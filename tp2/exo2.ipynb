{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process (MDP) with Two States\n",
    "\n",
    "In this exercise, we consider a system of two states \\( s_1 \\) and \\( s_2 \\). The system can transition between these states using two actions \\( a_1 \\) and \\( a_2 \\) with a discount factor \\( \\gamma = 0.8 \\). We are tasked with writing the Bellman optimality equations, solving them using the fixed-point iteration method, and deriving the optimal policies.\n",
    "\n",
    "## 1. Bellman Optimality Equations\n",
    "\n",
    "The Bellman optimality equations express the value of each state in terms of the expected values of the states it can transition to after taking certain actions. The equations are defined as follows:\n",
    "\n",
    "### For state \\( s_1 \\):\n",
    "\n",
    "\\[\n",
    "V*(s_1) = max{ Q*(s_1, a_1), Q*(s_1, a_2) }\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "\n",
    "\\[\n",
    "Q*(s_1, a_1) = 0.7 * (-1) + 0.3 * (1) + gamma ( 0.7 V*(s_1) + 0.3 V*(s_2) )\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "Q*(s_1, a_2) = 0.8 * (-1/2) + 0.2 * ( 3/2 ) + gamma ( 0.8 V*(s_1) + 0.2 V*(s_2) )\n",
    "\\]\n",
    "\n",
    "### For state \\( s_2 \\):\n",
    "\n",
    "\\[\n",
    "V*(s_2) = max{ Q*(s_2, a_1), Q*(s_2, a_2) }\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "\n",
    "\\[\n",
    "Q*(s_2, a_1) = 0.9 * (-2/3) + 0.1 * (5/4) + gamma ( 0.9 V*(s_1) + 0.1 V*(s_2) )\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "Q*(s_2, a_2) = 0.5 * (-1) + 0.5 * (1) + gamma ( 0.5 V*(s_1) + 0.5 V*(s_2) )\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 52 iterations\n",
      "Optimal Value Function: V*(s1) = -0.3947, V*(s2) = -0.2632\n"
     ]
    }
   ],
   "source": [
    "# Python Code to Compute the Value Functions\n",
    "import numpy as np\n",
    "\n",
    "# Define parameters\n",
    "gamma = 0.8  # Discount factor\n",
    "threshold = 1e-6  # Convergence threshold\n",
    "\n",
    "# Initialize value function for both states\n",
    "V = np.zeros(2)  # V[0] = V*(s1), V[1] = V*(s2)\n",
    "\n",
    "def compute_values(V):\n",
    "    # Calculate Q values based on current V\n",
    "    Q_a1_s1 = 0.7 * (-1) + 0.3 * (1) + gamma * (0.7 * V[0] + 0.3 * V[1])\n",
    "    Q_a2_s1 = 0.8 * (-0.5) + 0.2 * (1.5) + gamma * (0.8 * V[0] + 0.2 * V[1])\n",
    "    \n",
    "    Q_a1_s2 = 0.9 * (-2/3) + 0.1 * (5/4) + gamma * (0.9 * V[0] + 0.1 * V[1])\n",
    "    Q_a2_s2 = 0.5 * (-1) + 0.5 * (1) + gamma * (0.5 * V[0] + 0.5 * V[1])\n",
    "    \n",
    "    # Update V values\n",
    "    V_new = np.zeros(2)\n",
    "    V_new[0] = max(Q_a1_s1, Q_a2_s1)\n",
    "    V_new[1] = max(Q_a1_s2, Q_a2_s2)\n",
    "    \n",
    "    return V_new\n",
    "\n",
    "# Fixed-point iteration\n",
    "iteration = 0\n",
    "while True:\n",
    "    iteration += 1\n",
    "    V_new = compute_values(V)\n",
    "    if np.max(np.abs(V_new - V)) < threshold:\n",
    "        break\n",
    "    V = V_new\n",
    "\n",
    "print(f'Converged after {iteration} iterations')\n",
    "print(f'Optimal Value Function: V*(s1) = {V[0]:.4f}, V*(s2) = {V[1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Solving the System Using Fixed-Point Iteration\n",
    "\n",
    "The fixed-point iteration method is used to solve the Bellman optimality equations. We start with an arbitrary initialization for the value function and iteratively update the values based on the expected rewards and future values until convergence.\n",
    "\n",
    "In the code, we define a function `compute_values` to calculate the Q-values for actions \\( a_1 \\) and \\( a_2 \\) in both states. We then use a loop to update the value function until the maximum change is less than the defined threshold.\n",
    "\n",
    "The output shows the optimal value function for states \\( s_1 \\) and \\( s_2 \\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy: {'s1': 'a2', 's2': 'a2'}\n"
     ]
    }
   ],
   "source": [
    "# Derive the optimal policies based on the computed value function\n",
    "def derive_optimal_policy(V):\n",
    "    # Calculate Q values based on the optimal value function V\n",
    "    Q_a1_s1 = 0.7 * (-1) + 0.3 * (1) + gamma * (0.7 * V[0] + 0.3 * V[1])\n",
    "    Q_a2_s1 = 0.8 * (-0.5) + 0.2 * (1.5) + gamma * (0.8 * V[0] + 0.2 * V[1])\n",
    "    \n",
    "    Q_a1_s2 = 0.9 * (-2/3) + 0.1 * (5/4) + gamma * (0.9 * V[0] + 0.1 * V[1])\n",
    "    Q_a2_s2 = 0.5 * (-1) + 0.5 * (1) + gamma * (0.5 * V[0] + 0.5 * V[1])\n",
    "\n",
    "    # Determine the optimal policy\n",
    "    policy = {}\n",
    "    policy['s1'] = 'a1' if Q_a1_s1 > Q_a2_s1 else 'a2'\n",
    "    policy['s2'] = 'a1' if Q_a1_s2 > Q_a2_s2 else 'a2'\n",
    "\n",
    "    return policy\n",
    "\n",
    "optimal_policy = derive_optimal_policy(V)\n",
    "print(\"Optimal Policy:\", optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deriving the Optimal Policies\n",
    "\n",
    "After computing the optimal value function \\( V^*(s_1) \\) and \\( V^*(s_2) \\), we derive the optimal policy for each state by choosing the action that maximizes the expected value.\n",
    "\n",
    "For each state, we compare the Q-values for actions \\( a_1 \\) and \\( a_2 \\) and select the action that provides the maximum value. The resulting policy indicates the best action to take in each state to maximize the expected rewards.\n",
    "\n",
    "The output shows the optimal policy derived from the computed value function.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
