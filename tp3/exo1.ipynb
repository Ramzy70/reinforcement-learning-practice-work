{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawing the maze as a starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "maze_walls = [\n",
    "        # (Up, Right, Down, Left) walls for each cell\n",
    "        [(False, True, False, True), (True, True, False, True), (True, True, True, True), (True, True, False, True)],  # Row 0\n",
    "        [(False, False, True, True), (False, False, False, False), (True, False, True, False), (False, True, False, False)],  # Row 1\n",
    "        [(True, False, True, True), (False, True, True, False), (True, False, False, True), (False, True, True, False)],  # Row 2\n",
    "        [(True, False, True, True), (True, False, True, False), (False, False, False, False), (True, True, True, False)]   # Row 3\n",
    "    ]\n",
    "\n",
    "\n",
    "def draw_maze():\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    # Coordinates for the grid (0,0) at the top left, (3,3) at the bottom right\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            # Draw cell borders based on wall presence\n",
    "            if maze_walls[i][j][0]:  # Top wall\n",
    "                ax.plot([j, j + 1], [4 - i, 4 - i], color='black')\n",
    "            if maze_walls[i][j][1]:  # Right wall\n",
    "                ax.plot([j + 1, j + 1], [4 - i - 1, 4 - i], color='black')\n",
    "            if maze_walls[i][j][2]:  # Bottom wall\n",
    "                ax.plot([j, j + 1], [4 - i - 1, 4 - i - 1], color='black')\n",
    "            if maze_walls[i][j][3]:  # Left wall\n",
    "                ax.plot([j, j], [4 - i - 1, 4 - i], color='black')\n",
    "\n",
    "    # Mark the start and exit cells\n",
    "    ax.text(0.5, 3.5, 'S', fontsize=20, ha='center', va='center', color='green')  # Start\n",
    "    ax.text(2.5, 0.5, 'E', fontsize=20, ha='center', va='center', color='red')    # Exit\n",
    "\n",
    "    # Set limits and hide ticks\n",
    "    ax.set_xlim(0, 4)\n",
    "    ax.set_ylim(0, 4)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Hide the axes borders\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "\n",
    "    # Display the maze\n",
    "    plt.show()\n",
    "\n",
    "draw_maze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Formalizing the Problem as an MDP\n",
    "\n",
    "**States:** Each cell in the 2D grid is a state. The agent can be in any of these cells, with the start state being (0,0) and the exit state being (3,2). This gives us a total of  4×4=16 states.\n",
    "\n",
    "**Actions**: The agent can take four actions:\n",
    "\n",
    "Up\n",
    "Down\n",
    "Left\n",
    "Right\n",
    "\n",
    "**Transition Probabilities**: Movement is probabilistic. When the agent takes an action:\n",
    "\n",
    "It moves in the intended direction with probability 0.8.\n",
    "\n",
    "It moves in the opposite direction or stays in the same cell with probability 0.2, depending on whether the opposite direction is blocked by a wall.\n",
    "\n",
    "**Rewards**:\n",
    "\n",
    "+50 for reaching the exit (cell (3,2)).\n",
    "-10 for colliding with a wall.\n",
    "-1 for any movement that doesn’t lead to the exit.\n",
    "\n",
    "**Discount Factor**: γ=0.9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-3: The objective of penalizing the agent for executing actions\n",
    "\n",
    "Attempt to move into a wall (no permitted action) or Fail to reach the exit cell (just moving around) will encourage the agent to find an efficient and optimal path to the goal (the exit cell) rather than taking random or unnecessary actions. So it's for avoiding wasteful or random actions (hitting walls), and minimizing the time to reach the goal (reducing unnecessary movements)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-a: Policy Evaluation for π1 and π2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function for Policy 1 (π1):\n",
      "[[-0.24390244 -2.47834477 -0.24390244 -2.47834477]\n",
      " [-0.24390244 -1.43367043 -0.24390244 -1.43367043]\n",
      " [-0.24390244 -0.24390244 48.53658537 -0.24390244]\n",
      " [-0.24390244 -0.24390244  0.         -0.24390244]]\n",
      "\n",
      "Value Function for Policy 2 (π2):\n",
      "[[-9.99973439 -9.99973439 -9.99973439 -9.99973439]\n",
      " [-9.99973439 -9.99973439 -9.99973439 -9.99973439]\n",
      " [-9.99973439 -9.99973439 44.00002656 -9.99973439]\n",
      " [42.81970776 49.34065934  0.         -9.99973439]]\n"
     ]
    }
   ],
   "source": [
    "# Define transition actions\n",
    "transitions = {\n",
    "    'Up': (-1, 0),\n",
    "    'Right': (0, 1),\n",
    "    'Down': (1, 0),\n",
    "    'Left': (0, -1)\n",
    "}\n",
    "\n",
    "# Define rewards\n",
    "reward = np.full((4, 4), -1)  # Default penalty for all moves\n",
    "reward[3, 2] = 50  # Reward for reaching the exit\n",
    "\n",
    "# Initialize value function\n",
    "V_pi1 = np.zeros((4, 4))\n",
    "V_pi2 = np.zeros((4, 4))\n",
    "\n",
    "# Initial policies\n",
    "policy_1 = [\n",
    "    ['Right', 'Down', 'Down', 'Down'],  # Row 0\n",
    "    ['Down', 'Down', 'Down', 'Down'],    # Row 1\n",
    "    ['Down', 'Right', 'Down', 'Right'],  # Row 2\n",
    "    ['Down', 'Down', 'Down', 'Right']     # Row 3\n",
    "]\n",
    "\n",
    "policy_2 = [\n",
    "    [                                             # Row 0\n",
    "        {'Right': 0.8, 'Down': 0.2},               # (0, 0)\n",
    "        {'Down': 0.8, 'Right': 0.2},               # (0, 1)\n",
    "        {'Down': 0.8, 'Right': 0.2},               # (0, 2)\n",
    "        {'Down': 0.7, 'Right': 0.3}                # (0, 3)\n",
    "    ],\n",
    "    [                                             # Row 1\n",
    "        {'Right': 0.9, 'Down': 0.1},               # (1, 0)\n",
    "        {'Right': 0.8, 'Down': 0.2},               # (1, 1)\n",
    "        {'Right': 0.9, 'Down': 0.1},               # (1, 2)\n",
    "        {'Up': 0.2, 'Down': 0.8}                   # (1, 3)\n",
    "    ],\n",
    "    [                                             # Row 2\n",
    "        {'Right': 0.5, 'Down': 0.5},               # (2, 0)\n",
    "        {'Right': 1.0},                            # (2, 1)\n",
    "        {'Right': 0.1, 'Down': 0.9},               # (2, 2)\n",
    "        {'Right': 1.0}                             # (2, 3)\n",
    "    ],\n",
    "    [                                             # Row 3\n",
    "        {'Right': 0.9, 'Down': 0.1},               # (3, 0)\n",
    "        {'Right': 0.9, 'Down': 0.1},               # (3, 1)\n",
    "        {'Up': 0.2, 'Down': 0.8},                  # (3, 2)\n",
    "        {'Up': 0.2, 'Down': 0.8}                   # (3, 3)\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Helper function to check bounds\n",
    "def in_bounds(i, j):\n",
    "    return 0 <= i < 4 and 0 <= j < 4\n",
    "\n",
    "# Function to check valid moves considering walls\n",
    "def can_move(current, direction):\n",
    "    walls = maze_walls[current[0]][current[1]]\n",
    "    if direction == 'Up':\n",
    "        return not walls[0] and in_bounds(current[0] - 1, current[1])\n",
    "    elif direction == 'Right':\n",
    "        return not walls[1] and in_bounds(current[0], current[1] + 1)\n",
    "    elif direction == 'Down':\n",
    "        return not walls[2] and in_bounds(current[0] + 1, current[1])\n",
    "    elif direction == 'Left':\n",
    "        return not walls[3] and in_bounds(current[0], current[1] - 1)\n",
    "    return False\n",
    "\n",
    "# Policy evaluation for both π1 and π2\n",
    "def evaluate_policy(policy, V, gamma=0.9, iterations=100):\n",
    "    for _ in range(iterations):\n",
    "        new_V = np.copy(V)\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                if (i, j) == (3, 2):  # Skip exit cell\n",
    "                    continue\n",
    "                total_value = 0\n",
    "                if isinstance(policy[i][j], str):  # Deterministic (π1)\n",
    "                    action = policy[i][j]\n",
    "                    if can_move((i, j), action):\n",
    "                        ni, nj = i + transitions[action][0], j + transitions[action][1]\n",
    "                        total_value = 0.8 * (reward[ni, nj] + gamma * V[ni, nj])  # Intended action\n",
    "                    total_value += 0.2 * (reward[i, j] + gamma * V[i, j])  # Probability of staying\n",
    "                else:  # Stochastic (π2)\n",
    "                    for action, prob in policy[i][j].items():\n",
    "                        if can_move((i, j), action):\n",
    "                            ni, nj = i + transitions[action][0], j + transitions[action][1]\n",
    "                            total_value += prob * (reward[ni, nj] + gamma * V[ni, nj])\n",
    "                        else:\n",
    "                            total_value += prob * (reward[i, j] + gamma * V[i, j])\n",
    "                new_V[i, j] = total_value\n",
    "        V = new_V\n",
    "    return V\n",
    "\n",
    "# Evaluate both policies\n",
    "V_pi1 = evaluate_policy(policy_1, V_pi1, iterations=100)\n",
    "V_pi2 = evaluate_policy(policy_2, V_pi2, iterations=100)\n",
    "\n",
    "# Output the value functions\n",
    "print(\"Value Function for Policy 1 (π1):\")\n",
    "print(V_pi1)\n",
    "\n",
    "print(\"\\nValue Function for Policy 2 (π2):\")\n",
    "print(V_pi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "Policy 1: This is a deterministic policy, meaning the agent always takes a specific action in each state.\n",
    "\n",
    "Policy 2: This is a stochastic policy, where the agent chooses actions based on probabilities (e.g., 80% to move down, 20% to move left).\n",
    "\n",
    "We initialize the value function for each state as V(s)=0, and then iteratively update it based on the Bellman equation until the value function converges (i.e., changes in the value function are smaller than θ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-b: Policy Improvement for π1 and π2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Improvement Function: The policy_improvement function generates a new policy based on the current value function V. \n",
    "\n",
    "It evaluates all possible actions for each state and selects the action that yields the highest expected value.\n",
    "\n",
    "Skip Exit Cell: The exit cell (3,2) is skipped in the improvement step since the agent should already be there if it reaches that state.\n",
    "\n",
    "Execution: The improved policies for both π1 and π2 are calculated using their respective value functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Policy 1 (π1):\n",
      "['Down' 'Down' None 'Down']\n",
      "['Up' 'Right' 'Right' 'Down']\n",
      "['Right' 'Left' 'Down' 'Left']\n",
      "['Right' 'Right' '' 'Left']\n",
      "\n",
      "Improved Policy 2 (π2):\n",
      "['Down' 'Down' None 'Down']\n",
      "['Up' 'Up' 'Right' 'Up']\n",
      "['Right' 'Up' 'Down' 'Left']\n",
      "['Right' 'Right' '' 'Left']\n"
     ]
    }
   ],
   "source": [
    "# Policy improvement function\n",
    "def policy_improvement(V, gamma=0.9):\n",
    "    new_policy = np.full((4, 4), '', dtype=object)  # Initialize new policy\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            if (i, j) == (3, 2):  # Skip exit cell\n",
    "                continue\n",
    "            \n",
    "            best_action = None\n",
    "            best_value = float('-inf')\n",
    "            \n",
    "            for action in transitions.keys():\n",
    "                if can_move((i, j), action):\n",
    "                    ni, nj = i + transitions[action][0], j + transitions[action][1]\n",
    "                    action_value = reward[ni, nj] + gamma * V[ni, nj]\n",
    "                    \n",
    "                    # Choose the action with the highest value\n",
    "                    if action_value > best_value:\n",
    "                        best_value = action_value\n",
    "                        best_action = action\n",
    "            \n",
    "            new_policy[i, j] = best_action  # Update policy with the best action\n",
    "    return new_policy\n",
    "\n",
    "# Policy Improvement for π1\n",
    "improved_policy_1 = policy_improvement(V_pi1)\n",
    "\n",
    "# Policy Improvement for π2\n",
    "improved_policy_2 = policy_improvement(V_pi2)\n",
    "\n",
    "# Output the improved policies\n",
    "print(\"Improved Policy 1 (π1):\")\n",
    "for row in improved_policy_1:\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nImproved Policy 2 (π2):\")\n",
    "for row in improved_policy_2:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Policy Iteration Algorithm\n",
    "\n",
    "Policy Iteration ( using iterative policy evaluation) for estimating π =(equal nearly) π* \n",
    "\n",
    "1.\tInitialization\n",
    "\n",
    "V(s) E R and π(s) E A(s) arbitrarily for all s E S; V(terminal)= 0 \n",
    "\n",
    "2.\tPolicy Evaluation\n",
    "\n",
    "Loop:\n",
    "\n",
    "Delta= 0 \n",
    "\n",
    "Loop for each s E S: \n",
    "      v=V(s) \n",
    "      V(s) =Σs',r  p(s',r|s,π(s))[r+gammaV(s')] \n",
    "      Delta = max(Delta, | v -V(s)|) \n",
    "\n",
    "until Delta < theta (a small positive number determining the accuracy of estimation) \n",
    "\n",
    "3. Policy Improvement\n",
    "\n",
    "policy_stable = true \n",
    "\n",
    "For each s E S:\n",
    "\n",
    "      old-action  = π(s)\n",
    "      π(s) = argmax a Σs',r   p(s',r | s,a)[r +gammaV(s')] \n",
    "      If old_action =/= π(s), then policy_stable = False \n",
    "\n",
    "If policy_stable, then stop and return V =(equal nearly) v* and π(s) 􀀂 π(s)*; else go to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (Iterative):\n",
      "['Down' 'Down' None 'Down']\n",
      "['Right' 'Right' 'Right' 'Down']\n",
      "['Right' 'Up' 'Down' 'Left']\n",
      "['Right' 'Right' 'Down' 'Left']\n",
      "\n",
      "Optimal Value Function (Iterative):\n",
      "[[21.88646 25.4294  -1.      33.74   ]\n",
      " [25.4294  29.366   33.74    38.6    ]\n",
      " [21.88646 25.4294  50.      44.     ]\n",
      " [44.      50.       0.      50.     ]]\n"
     ]
    }
   ],
   "source": [
    "# Policy Iteration Algorithm\n",
    "def policy_iteration_iterative(initial_policy, theta=1e-5, gamma=0.9):\n",
    "    V = np.zeros((4, 4))  # Initialize value function\n",
    "    policy = np.array(initial_policy, dtype=object)  # Ensure policy is a NumPy array\n",
    "\n",
    "    while True:\n",
    "        # Policy Evaluation\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for i in range(4):\n",
    "                for j in range(4):\n",
    "                    if (i, j) == (3, 2):  # Skip exit cell\n",
    "                        continue\n",
    "\n",
    "                    v = V[i, j]\n",
    "                    action = policy[i, j]  # Use action from policy\n",
    "                    if can_move((i, j), action):\n",
    "                        ni, nj = i + transitions[action][0], j + transitions[action][1]\n",
    "                        V[i, j] = reward[ni, nj] + gamma * V[ni, nj]\n",
    "                    else:\n",
    "                        V[i, j] = -1  # Handle non-permitted action as -1 (or the default penalty)\n",
    "\n",
    "                    delta = max(delta, abs(v - V[i, j]))\n",
    "\n",
    "            if delta < theta:  # Convergence check\n",
    "                break\n",
    "\n",
    "        # Policy Improvement\n",
    "        policy_stable = True\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                if (i, j) == (3, 2):  # Skip exit cell\n",
    "                    continue\n",
    "\n",
    "                old_action = policy[i, j]\n",
    "                best_action = None\n",
    "                best_value = float('-inf')\n",
    "\n",
    "                for action in transitions.keys():\n",
    "                    if can_move((i, j), action):\n",
    "                        ni, nj = i + transitions[action][0], j + transitions[action][1]\n",
    "                        action_value = reward[ni, nj] + gamma * V[ni, nj]\n",
    "\n",
    "                        # Select the action with the maximum value\n",
    "                        if action_value > best_value:\n",
    "                            best_value = action_value\n",
    "                            best_action = action\n",
    "\n",
    "                policy[i, j] = best_action  # Update policy\n",
    "\n",
    "                if old_action != best_action:\n",
    "                    policy_stable = False  # Policy has changed\n",
    "\n",
    "        if policy_stable:\n",
    "            break  # If the policy is stable, exit the loop\n",
    "\n",
    "    return policy, V\n",
    "\n",
    "# Starting with the initial policy π1 for policy iteration\n",
    "optimal_policy_iterative, optimal_value_iterative = policy_iteration_iterative(policy_1)\n",
    "\n",
    "# Output the optimal policy and value function\n",
    "print(\"Optimal Policy (Iterative):\")\n",
    "for row in optimal_policy_iterative:\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nOptimal Value Function (Iterative):\")\n",
    "print(optimal_value_iterative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6: the value iteration algorithm \n",
    "\n",
    "Value Iteration, for estimating π =(equal nearly) π*\n",
    "\n",
    "Algorithm parameter: a small threshold theta > 0 determining accuracy of estimation  \n",
    "\n",
    "Initialize V( s), for all s E S+, arbitrarily except that V(terminal) = 0  \n",
    "\n",
    "Loop: \n",
    "\n",
    "Delta = 0\n",
    "\n",
    "       Loop for each s E S:\n",
    "       v = V(S)\n",
    "       V(S) = max a Σs',r  p(s',r|s,a)[r+gammaV(s')] \n",
    "       Delta = max(delta , |v-V(S)|)\n",
    "\n",
    "until Delta <0\n",
    "\n",
    "Output a deterministic policy, π =(equal nearly) π*, such that \n",
    "\n",
    "π(s) = argmax a Σs',r  p(s',r|s,a)[r+gammaV(s')]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function (Value Iteration):\n",
      "[[21.88646 25.4294      -inf 33.74   ]\n",
      " [25.4294  29.366   33.74    38.6    ]\n",
      " [21.88646 25.4294  50.      44.     ]\n",
      " [44.      50.       0.      50.     ]]\n",
      "\n",
      "Optimal Policy (Value Iteration):\n",
      "['Down' 'Down' None 'Down']\n",
      "['Right' 'Right' 'Right' 'Down']\n",
      "['Right' 'Up' 'Down' 'Left']\n",
      "['Right' 'Right' None 'Left']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_26704\\904904244.py:29: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  delta = max(delta, abs(v - V[i, j]))  # Update delta\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(theta=1e-5, gamma=0.9):\n",
    "    V = np.zeros((4, 4))  # Initialize the value function\n",
    "    policy = np.full((4, 4), None)  # Initialize policy\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # Loop over each state\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                if (i, j) == (3, 2):  # Skip exit cell\n",
    "                    continue\n",
    "                \n",
    "                v = V[i, j]  # Store old value\n",
    "                best_value = float('-inf')\n",
    "                best_action = None\n",
    "                \n",
    "                # Loop over each action\n",
    "                for action in transitions.keys():\n",
    "                    if can_move((i, j), action):\n",
    "                        ni, nj = i + transitions[action][0], j + transitions[action][1]\n",
    "                        action_value = reward[ni, nj] + gamma * V[ni, nj]\n",
    "                        \n",
    "                        # Update best value and best action\n",
    "                        if action_value > best_value:\n",
    "                            best_value = action_value\n",
    "                            best_action = action\n",
    "                \n",
    "                V[i, j] = best_value  # Update value function\n",
    "                delta = max(delta, abs(v - V[i, j]))  # Update delta\n",
    "\n",
    "        # Check convergence\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # Derive the policy from the value function\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            if (i, j) == (3, 2):  # Skip exit cell\n",
    "                continue\n",
    "            \n",
    "            best_value = float('-inf')\n",
    "            for action in transitions.keys():\n",
    "                if can_move((i, j), action):\n",
    "                    ni, nj = i + transitions[action][0], j + transitions[action][1]\n",
    "                    action_value = reward[ni, nj] + gamma * V[ni, nj]\n",
    "                    \n",
    "                    if action_value > best_value:\n",
    "                        best_value = action_value\n",
    "                        policy[i, j] = action  # Update policy\n",
    "\n",
    "    return V, policy\n",
    "\n",
    "# Run value iteration to get optimal value and policy\n",
    "optimal_value_function, optimal_policy_value_iteration = value_iteration()\n",
    "\n",
    "# Output the optimal value function and policy\n",
    "print(\"Optimal Value Function (Value Iteration):\")\n",
    "print(optimal_value_function)\n",
    "\n",
    "print(\"\\nOptimal Policy (Value Iteration):\")\n",
    "for row in optimal_policy_value_iteration:\n",
    "    print(row)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
